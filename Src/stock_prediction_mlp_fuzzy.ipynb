{"cells":[{"cell_type":"markdown","id":"57276830","metadata":{},"source":["# üìà MLP-based Stock Price Prediction with MCES & Fuzzy Logic"]},{"cell_type":"code","execution_count":49,"id":"3f00c19b","metadata":{},"outputs":[],"source":["# üõ†Ô∏è Imports & Setup\n","import os, pickle\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","from datetime import date\n","from tqdm.auto import tqdm\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error"]},{"cell_type":"code","execution_count":50,"id":"1a8e65e9","metadata":{},"outputs":[],"source":["# ‚öôÔ∏è Configurations\n","ticker = 'C38U.SI'\n","PRED_PERIOD = 13\n","PERIOD = 14\n","wkdir = f'/home/priya/Desktop/fyp/Src alwin/Src/data/C38U.SI'\n","MCES_START = date(1988, 1, 1)\n","MCES_END = date(2018, 1, 1)\n","\n","\n","from datetime import date \n","\n","INITIALIZATION_MIN = 5\n","\n","CLUSTER_MAXCLUSTERS = 15\n","CLUSTER_MINCLUSTERS = 15\n","FACTOR = 2\n","\n","MERGED_MIN_STD = 2\n","MERGED_MIN = 4\n","MERGED_MAX = 8\n","\n","MERGED_YMAX = 8\n","MERGED_MINY_STD = 2\n","\n","\n","MAX_LAYERS = 4\n","\n","MCES_START = date(1988, 1, 1)\n","MCES_END = date(2018, 1, 1)\n","\n","ZERO_REPLACEMENT = 0.0001 \n","PERIOD = 14\n","PRED_PERIOD = 13\n","TEST_START = date(2019, 1, 1)\n","TEST_END = date(2024, 1, 1)\n","VAL_START = date(2018, 1, 1)"]},{"cell_type":"code","execution_count":51,"id":"0668b010","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Data & cluster details loaded\n"]}],"source":["# üì• Load Data\n","ftraindf = pd.read_csv(f'/home/priya/Desktop/fyp/Src alwin/Src/data/C38U.SI/ftraindf.csv')\n","fvaldf = pd.read_csv(f'/home/priya/Desktop/fyp/Src alwin/Src/data/C38U.SI/fvaldf.csv')\n","ftestdf = pd.read_csv(f'/home/priya/Desktop/fyp/Src alwin/Src/data/C38U.SI/ftestdf.csv')\n","\n","with open(f'/home/priya/Desktop/fyp/Src alwin/Src/data/{ticker}/cluster_details.pkl', 'rb') as handle:\n","    cluster_details = pickle.load(handle)\n","\n","print(\"‚úÖ Data & cluster details loaded\")"]},{"cell_type":"code","execution_count":52,"id":"045181d0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Loaded feature selection for C38U.SI\n"]}],"source":["# üìä Load Selected Features\n","features_path = f\"/home/priya/Desktop/fyp/Src alwin/Src/data/{ticker}/features_selected.pkl\"\n","with open(features_path, \"rb\") as handle:\n","    features_selected = pickle.load(handle)\n","print(f\"‚úÖ Loaded feature selection for {ticker}\")"]},{"cell_type":"code","execution_count":53,"id":"7c8d8573","metadata":{},"outputs":[],"source":["#  Define Model Configurations\n","model_dict = {\n","    plus_target: {\n","        'model_type': 'mlp',\n","        'mlp': {\n","            'layers': {\n","                0: {'nodes': 256},\n","                1: {'nodes': 64},\n","                2: {'nodes': 128},\n","            },\n","            'hl_activation': 'relu',\n","            'ol_activation': 'sigmoid',\n","            'optimizer': {\n","                'optim_type': 'adam',\n","                'learning_rate': 0.001,\n","            },\n","            'shuffle': False,\n","            'verbose': 0,\n","        },\n","        'early_stopper': {\n","            'patience': 5,\n","            'min_delta': 0\n","        },\n","        'epochs': 20,\n","        'batch_size': 16,\n","    }\n","    for plus_target in range(1, PRED_PERIOD + 1)\n","}\n"]},{"cell_type":"code","execution_count":54,"id":"e3425bc5","metadata":{},"outputs":[],"source":["import pandas as pd \n","from sklearn.metrics import r2_score, mean_squared_error\n","import sys, os \n","sys.path.append(os.getcwd())\n","from s1_data_preparation.config import *\n","\n","def evaluateModel(outcome, trainset, valset, testset, cluster_details, header, plus_target): \n","    \n","    eval_res = {}\n","    print(\"Starting model evaluation...\")\n","    \n","    for result in outcome: \n","        print(f\"Processing result: {result}\")\n","\n","        # if result == 'train_pred': continue\n","\n","        mode = 0\n","\n","        ## normalization \n","        pred_res = outcome[result].copy()\n","        print(f\"Initial prediction results for {result}:\\n\", pred_res.head())\n","        \n","        if result == 'val_pred': \n","            reference = valset['ref']\n","        elif result == 'test_pred': \n","            reference = testset['ref']\n","        else: \n","            reference = trainset['ref']\n","        print(f\"Reference data for {result}:\\n\", reference.head())\n","        \n","        cols = list(pred_res.columns)\n","        print(f\"Prediction columns: {cols}\")\n","\n","        if mode == 0: \n","            print(\"Applying normalization mode 0...\")\n","            \n","            pred_res['summation'] = pred_res.apply(lambda x: sum(x), axis=1)\n","            for col in cols: \n","                pred_res[col] = pred_res.apply(lambda x: round(x[col]/x['summation'], 6), axis=1)\n","            \n","            pred_res = pred_res.drop(['summation'], axis=1) \n","            print(f\"Normalized predictions: \\n\", pred_res.head())\n","\n","        if mode == 1: \n","            print(\"Applying normalization mode 1...\")\n","            pred_res['maximum'] = pred_res.apply(lambda x: max(x), axis=1)\n","            for col in cols: \n","                pred_res[col] = pred_res.apply(lambda x: 1 if x[col] == x['maximum'] else 0, axis=1)\n","            \n","        ## defuzzify \n","        print(\"Performing defuzzification...\")\n","        pred_res['pc_pred'] = 0 \n","        for col in cols: \n","            pred_res['pc_pred'] = pred_res.apply(lambda x: x['pc_pred'] + x[col]*cluster_details[header][col]['mean'], axis=1)\n","        print(f\"Defuzzified predictions: \\n\", pred_res.head())\n","        \n","        # price change \n","        print(\"Calculating price predictions...\")\n","        price_pred = pd.concat([pred_res, reference[[f'refPrice_Tm{period}', f'yref_Tp{plus_target}_Price', f'ypcref_Tp{plus_target}_PriceChg']]], axis=1)\n","        price_pred['price_pred'] = price_pred.apply(lambda x: x[f'refPrice_Tm{period}']*(1+x['pc_pred']), axis=1) \n","        price_pred = price_pred[[f'yref_Tp{plus_target}_Price', 'price_pred', f'ypcref_Tp{plus_target}_PriceChg', 'pc_pred']]\n","        price_pred['error'] = price_pred.apply(lambda x: abs(x['price_pred'] - x[f'yref_Tp{plus_target}_Price'])/x[f'yref_Tp{plus_target}_Price'], axis=1)\n","        print(f\"Price prediction results: \\n\", price_pred.head())\n","\n","        pred_r2 = r2_score(price_pred[f'yref_Tp{plus_target}_Price'], price_pred['price_pred'])\n","        pred_rmse = mean_squared_error(price_pred[f'yref_Tp{plus_target}_Price'], price_pred['price_pred'])**0.5\n","        print(f\"Metrics - RMSE: {pred_rmse}, R2: {pred_r2}, MAPE: {price_pred['error'].mean()}\")\n","\n","        eval_res[result] = {\n","            'rmse': pred_rmse, \n","            'r2': pred_r2, \n","            'mape': price_pred['error'].mean(), \n","            'predicted': reference, \n","            'ref': price_pred, \n","            'cluster_ref': pred_res\n","        }\n","    \n","    print(\"Model evaluation completed.\")\n","    return eval_res"]},{"cell_type":"code","execution_count":55,"id":"8e4398cc","metadata":{},"outputs":[],"source":["\n","from keras.models import Sequential\n","from keras.layers import Dense, SimpleRNN\n","from keras.callbacks import EarlyStopping\n","import pandas as pd \n","\n","\n","def mlpConstructor(model_dict): \n","\n","    nn = Sequential()\n","\n","    for layer in model_dict['mlp']['layers']: \n","\n","        nodes = model_dict['mlp']['layers'][layer]['nodes']\n","        activation = model_dict['mlp']['hl_activation']\n","\n","        if layer == 0: nn.add(Dense(nodes, input_dim = model_dict['input_size'], activation = activation))\n","        else: nn.add(Dense(nodes, activation = activation))\n","    \n","    # add output layer \n","    activation = model_dict['mlp']['ol_activation']\n","    nn.add(Dense(model_dict['output_size'], activation = activation))    \n","    \n","    # compile model \n","    if activation == 'linear': nn.compile(loss = 'mse', optimizer = model_dict['mlp']['optimizer']['optim_type'])\n","    # elif activation == 'sigmoid': nn.compile(loss = 'binary_crossentropy', optimizer =  model_dict['mlp']['optimizer']['optim_type'], metrics = ['binary_accuracy'])\n","    # elif activation == 'softmax': nn.compile(loss = 'categorical_crossentropy', optimizer =  model_dict['mlp']['optimizer']['optim_type'], metrics = ['accuracy'])\n","    elif activation == 'sigmoid': nn.compile(loss = 'mse', optimizer =  model_dict['mlp']['optimizer']['optim_type'])\n","    elif activation == 'softmax': nn.compile(loss = 'categorical_crossentropy', optimizer =  model_dict['mlp']['optimizer']['optim_type'], metrics = ['accuracy'])\n","\n","\n","    return nn\n","\n","# kerasTrain function \n","# kerasTrain function\n","def kerasTrain(model, model_dict, cluster_details, trainset, valset, testset, header):\n","\n","    es = None \n","    # early stopper\n","    es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience = model_dict['early_stopper']['patience'], min_delta=model_dict['early_stopper']['min_delta'])\n","\n","    # model fit\n","    if es!= None: model.fit(trainset['X'], trainset['y'], epochs = model_dict['epochs'], batch_size = model_dict['batch_size'], shuffle = model_dict['mlp']['shuffle'], verbose = model_dict['mlp']['verbose'], validation_data = (valset['X'], valset['y']), callbacks=[es])\n","    else: model.fit(trainset['X'], trainset['y'], epochs = model_dict['epochs'], batch_size = model_dict['batch_size'], shuffle = model_dict['mlp']['shuffle'], verbose = model_dict['mlp']['verbose'], validation_data = (valset['X'], valset['y']))\n","    val_pred = model.predict(valset['X'], verbose = model_dict['mlp']['verbose'])\n","    test_pred = model.predict(testset['X'], verbose = model_dict['mlp']['verbose'])\n","\n","    val_pred = pd.DataFrame(val_pred, columns = valset['y'].columns, index = valset['y'].index)\n","    test_pred = pd.DataFrame(test_pred, columns = testset['y'].columns, index = testset['y'].index)\n","\n","    return model, val_pred, test_pred\n"]},{"cell_type":"code","execution_count":56,"id":"a6be149b","metadata":{},"outputs":[],"source":["\n","\n","import sys, os, copy, random\n","import pandas as pd \n","import torch, pickle\n","import gc\n","\n","# from tqdm import tqdm \n","from sklearn.linear_model import LinearRegression \n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n","from tqdm.auto import tqdm\n","\n","sys.path.append(os.getcwd())\n","from s1_data_preparation.config import *\n","from s2_crystal_ball.config import *\n","from s2_crystal_ball.keras_neural import mlpConstructor, kerasTrain\n","from s2_crystal_ball.pytorch_neural import Transformer, pytorchTrain, MLP\n","from s2_crystal_ball.evaluation import evaluateModel\n","from s2_crystal_ball.mces import mcesPipeline\n","\n","def neuralDataPreparation(ftraindf, fvaldf, ftestdf, period = PERIOD, plus_target = 1): \n","\n","    x_cols = [col for col in ftraindf.columns if 'x_' in col]\n","    y_cols = [col for col in ftraindf.columns if f'y_Tp{plus_target}_PriceChg_' in col]\n","    ref_cols = [col for col in ftraindf.columns if f'yref_Tp{plus_target}_Price' in col or f'refPrice_Tm{period}' in col or f'ypcref_Tp{plus_target}_PriceChg' in col or f'yref_Tp{plus_target}_Date' in col or 'yref_Tm0_close' in col]\n","\n","\n","    trainset = {'X': ftraindf[x_cols], 'y': ftraindf[y_cols], 'ref': ftraindf[ref_cols]}\n","    valset = {'X': fvaldf[x_cols], 'y': fvaldf[y_cols], 'ref': fvaldf[ref_cols]}\n","    testset = {'X': ftestdf[x_cols], 'y': ftestdf[y_cols], 'ref': ftestdf[ref_cols]}\n","    \n","    return trainset, valset, testset    \n","\n","\n","def neuralConstructor(model_dict): \n","\n","    model = None \n","\n","    if model_dict['model_type'] == 'mlp': model = mlpConstructor(model_dict)\n","    # elif model_dict['model_type'] == 'rnn': model = rnnConstructor(model_dict)\n","    elif model_dict['model_type'] == 'transformer': model = Transformer(model_dict)\n","\n","    return model \n","\n","\n","def monoNeuralPipeline(ftraindf, fvaldf, ftestdf, model_dict, cluster_details, plus_target=1, chosen_features=None, mode=0):\n","    import torch, gc, pickle\n","\n","    header = f'y_Tp{plus_target}_PriceChg'\n","\n","    # Step 1: Feature selection and data preparation\n","    if mode == 1:\n","        rftraindf, rfvaldf, rftestdf, featureSelection = mces(\n","            ftraindf, fvaldf, ftestdf, cluster_details, plus_target, chosen_features\n","        )\n","        trainset, valset, testset = neuralDataPreparation(rftraindf, rfvaldf, rftestdf, plus_target=plus_target)\n","    else:\n","        trainset, valset, testset = neuralDataPreparation(ftraindf, fvaldf, ftestdf, plus_target=plus_target)\n","\n","    # Step 2: Update model_dict[plus_target]\n","    model_dict['input_size'] = len(trainset['X'].columns)\n","    model_dict['output_size'] = len(trainset['y'].columns)\n","    model_dict['day_target'] = plus_target\n","\n","    model_type = model_dict.get('model_type', 'transformer')\n","\n","    # Step 3: Build model\n","    if model_type == 'transformer':\n","        input_size = model_dict['input_size']\n","        # Find valid nhead values that divide input_size\n","        nhead_options = [i for i in range(1, 11) if input_size % i == 0]\n","        nhead = max(nhead_options) if nhead_options else 1\n","\n","        # Ensure d_model is divisible by nhead\n","        d_model = input_size if input_size % nhead == 0 else ((input_size // nhead) + 1) * nhead\n","\n","        # Update model_dict transformer settings\n","        model_dict['transformer']['nhead'] = nhead\n","        model_dict['transformer']['d_model'] = d_model\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model = neuralConstructor(model_dict).to(device)\n","\n","        # Train using PyTorch\n","        model, train_pred, val_pred, test_pred = pytorchTrain(\n","            model=model,\n","            model_dict=model_dict,\n","            trainset=trainset,\n","            valset=valset,\n","            testset=testset,\n","            traintype=0\n","        )\n","    else:\n","        # MLP or other models (using Keras backend)\n","        model = neuralConstructor(model_dict)\n","        model, train_pred, val_pred, test_pred = kerasTrain(\n","            model, model_dict, cluster_details, trainset, valset, testset, header\n","        )\n","\n","    # Step 4: Store predictions\n","    outcome = {\n","        'train_pred': train_pred,\n","        'val_pred': val_pred,\n","        'test_pred': test_pred\n","    }\n","\n","    # Step 5: Reload cluster details\n","    ticker = 'C38U.SI'\n","    with open(f'/home/priya/Desktop/fyp/Src alwin/Src/data/{ticker}/cluster_details.pkl', 'rb') as handle:\n","        cluster_details = pickle.load(handle)\n","    print(\"Reloaded cluster_details\")\n","\n","    # Step 6: Evaluate model\n","    eval_res = evaluateModel(\n","        outcome=outcome,\n","        trainset=trainset,\n","        valset=valset,\n","        testset=testset,\n","        cluster_details=cluster_details,\n","        header=header,\n","        plus_target=plus_target\n","    )\n","    eval_res['header'] = header\n","\n","    gc.collect()\n","\n","    return model, eval_res\n","\n","\n","\n","def optimizeMonoNetwork(ftraindf, fvaldf, ftestdf, cluster_details, plus_target): \n","\n","    model_type = 'transformer'\n","\n","    configurations = generateConfigurations(model_type, ftraindf)\n","    \n","    models = [] \n","    val_performance_r2 = []\n","    val_performance_rmse = []\n","    test_performance_r2 = []     \n","    test_performance_rmse = [] \n","    \n","    for configuration in tqdm(configurations): \n","        print(configuration)\n","        model, eval_res = monoNeuralPipeline(ftraindf, fvaldf, ftestdf, configuration, cluster_details, plus_target)\n","        models.append(model)\n","        val_performance_r2.append(eval_res['val_pred']['r2'])\n","        val_performance_rmse.append(eval_res['val_pred']['rmse'])\n","        test_performance_r2.append(eval_res['test_pred']['r2'])\n","        test_performance_rmse.append(eval_res['test_pred']['rmse'])\n","        \n","        # print(f'Config {configurations.index(configuration)} - val_r2: {eval_res['val_pred']['r2']}, val_rmse: {eval_res['val_pred']['rmse']} | test_r2: {eval_res['test_pred']['r2']}, test_rmse: {eval_res['test_pred']['rmse']}')\n","\n","    \n","    index = val_performance_rmse.index(max(val_performance_rmse))\n","    optimal_config = configurations[index]\n","    optimal_model = models[index]\n","    print(f'Optimal Config {index} - val_r2: {val_performance_r2[index]}, val_rmse: {val_performance_rmse[index]} | test_r2: {test_performance_r2[index]}, test_rmse: {test_performance_rmse[index]}')\n","    \n","    return optimal_model, optimal_config\n","\n","def generateConfigurations(model_type, ftraindf):\n","\n","    configurations = []\n","\n","    x_cols = [col for col in ftraindf if 'x_' in col]\n","\n","\n","    if model_type == 'transformer': \n","        template = {\n","            'model_type': 'transformer', \n","            'transformer':{\n","        #         'd_model': \n","                # 'nheads': 1, \n","        #         'dim_feedforward': {64, 128, 256, 512}, \n","        #         'num_encoder_layers': {2, 4, 8, 16}, \n","                'shuffle': False, \n","                'optimizer': {\n","                    'optim_type': 'adam', \n","                    'learning_rate': 0.001, \n","                },         \n","            }, \n","            'early_stopper': {\n","                'patience': 10, \n","                'min_delta': 0,\n","            },     \n","            'epochs': 100,\n","        #     'batch_size': {32, 64, 128, 256},\n","        }\n","\n","        dim_feedforward = [128, 256] # [64, 128, 256, 512]\n","        num_encoder_layers = [2, 4, 6] # [2, 4, 6, 8]\n","        batch_sizes = [32, 64, 128] # [32, 64, 128, 256]\n","        nheads = [i for i in range(1, 9) if len(x_cols)%i == 0]\n","\n","        for dim in dim_feedforward: \n","            for encoder_layer in num_encoder_layers: \n","                for batch_size in batch_sizes: \n","                    for nhead in nheads: \n","                        instance = copy.deepcopy(template)\n","                        \n","                        instance[instance['model_type']]['dim_feedforward'] = dim\n","                        instance[instance['model_type']]['num_encoder_layers'] = encoder_layer\n","                        instance[instance['model_type']]['nheads'] = nhead\n","                        instance['batch_size'] = batch_size\n","                        \n","                        configurations.append(instance)\n","\n","    return configurations\n","\n","\n","\n","def mcesDefuzzy(m1_pred, header, y_cols, cluster_details): \n","\n","    m1_pred['minimum'] = m1_pred.apply(lambda x: min(x), axis = 1)\n","    for col in y_cols: m1_pred[col] = m1_pred.apply(lambda x: round((x[col] - x['minimum']), 6), axis = 1)\n","\n","    m1_pred['summation'] = m1_pred.apply(lambda x: sum(x), axis = 1)\n","    for col in y_cols: m1_pred[col] = m1_pred.apply(lambda x: round(x[col]/x['summation'], 6), axis = 1)\n","\n","    m1_pred = m1_pred.drop(['minimum', 'summation'], axis = 1) \n","\n","    m1_pred['pc_pred'] = 0 \n","    for col in y_cols: m1_pred['pc_pred'] = m1_pred.apply(lambda x: x['pc_pred'] + x[col]*cluster_details[header][col]['mean'], axis = 1)\n","\n","    return m1_pred['pc_pred']\n","\n","def mces(ftraindf, fvaldf, ftestdf, cluster_details, plus_target, chosen_features = None, threshold = 10, iterations = 0): \n","\n","    print('[mces initiated] training mlp model ...')\n","\n","    price_features = [col for col in list(cluster_details.keys()) if 'Price' in col and 'x_' in col]\n","    x_cols = [col for col in ftraindf.columns if 'x_' in col]\n","    remainder_cols = [col for col in ftraindf.columns if col not in x_cols]\n","\n","    if chosen_features == None: \n","\n","        # define MLP Model \n","        model_dict = {\n","            'model_type': 'mlp', \n","            'mlp': {\n","                'layers': {\n","                    0: {'nodes': 256}, \n","                    1: {'nodes': 64}, \n","                    2: {'nodes': 128}, \n","                }, \n","                'hl_activation': 'relu', \n","                'ol_activation': 'sigmoid',\n","                'optimizer': {\n","                    'optim_type': 'adam', \n","                    'learning_rate': 0.001,\n","                }, \n","                'shuffle': False, \n","                'verbose': 0,\n","            },\n","            'early_stopper': {\n","                'patience': 5, \n","                'min_delta': 0        \n","            },\n","            'epochs': 20,\n","            'batch_size': 16,\n","        }\n","\n","        # train a model with full features \n","        model, eval_res, nothing = monoNeuralPipeline(ftraindf, fvaldf, ftestdf, model_dict, cluster_details, plus_target, mode = 0)\n","\n","        x_features = [col for col in list(cluster_details.keys()) if 'x_' in col and col not in price_features]\n","        y_cols = [col for col in ftraindf.columns if f'y_Tp{plus_target}_' in col]\n","        header = f'y_Tp{plus_target}_PriceChg'\n","        target = ftraindf[f'ypcref_Tp{plus_target}_PriceChg']\n","\n","        X_mces = ftraindf[x_cols].reset_index(drop = True)\n","\n","        mces_df = pd.DataFrame({'cols': x_features})\n","        scores = [0 for item in range(len(x_features))] \n","        frequency = [0 for item in range(len(x_features))] \n","        in_mask = [0 for item in range(len(x_features))] \n","        \n","        if iterations == 0: iterations = len(X_mces.index)\n","        # print(f'xcols len: {len(x_cols)}')\n","        for row_index in tqdm(range(iterations)): \n","\n","            temp = X_mces.copy()[X_mces.index == row_index]\n","            temp_target = pd.Series(target[row_index])\n","\n","            mask1 = [random.randint(0, 1) for col in range(len(x_features))]\n","\n","            # in case no more features left \n","            while sum(mask1) < 7: mask1 = [random.randint(0, 1) for col in range(len(x_features))]\n","            mces_df['mask1'] = mask1\n","\n","            disabled = list(mces_df[mces_df['mask1'] == 0]['cols'])\n","            enabled = list(mces_df[mces_df['mask1'] == 1]['cols'])\n","\n","            inverted = random.choice(enabled)\n","            inverted_index = x_features.index(inverted)\n","\n","            # retrieve respective cols \n","\n","            for col in disabled: \n","                for cluster in list(cluster_details[col].keys()): \n","                    temp[cluster] = 0\n","            for col in enabled: in_mask[x_features.index(col)] += 1\n","            frequency[inverted_index] += 1\n","\n","            # mask 1 \n","            \n","\n","            pred = model.predict(temp, verbose = model_dict['mlp']['verbose'])\n","            # print(f'Pred shape: {pred.shape}')\n","            # print(y_cols)\n","\n","            m1_pred = pd.DataFrame(pred, columns=y_cols)\n","            m1_predp = mcesDefuzzy(m1_pred, header, y_cols, cluster_details)\n","            rmse1 = mean_squared_error(temp_target, m1_predp)\n","\n","            for cluster in list(cluster_details[inverted].keys()): \n","                temp[cluster] = X_mces[cluster].mean()\n","\n","            m2_pred = pd.DataFrame(model.predict(temp, verbose = model_dict['mlp']['verbose']), columns=y_cols)\n","            m2_predp = mcesDefuzzy(m2_pred, header, y_cols, cluster_details)\n","            rmse2 = mean_squared_error(temp_target, m2_predp)\n","\n","            rmseDiff = rmse1 - rmse2\n","\n","            scores[inverted_index] += rmseDiff\n","            \n","            # if row_index > 3: break\n","\n","        mces_df['scores'] = scores\n","        mces_df['in_mask_freq'] = in_mask\n","        mces_df['frequency'] = frequency\n","        mces_df = mces_df.drop(['mask1'], axis = 1)\n","        \n","        try: \n","            mces_df['weighted_scores'] = mces_df.apply(lambda x: x['scores']/x['frequency'], axis = 1)\n","        except: \n","            mces_df['weighted_scores'] = mces_df.apply(lambda x: x['scores'], axis = 1)\n","        mces_df = mces_df.sort_values(by = ['weighted_scores'], ascending = False)    \n","        \n","        top_features = list(mces_df[mces_df['weighted_scores'] > 0]['cols'])\n","#         if len(top_features) < threshold: top_features = list(mces_df.head(threshold)['cols'])\n","        \n","        top_features += price_features\n","    \n","    else: top_features = chosen_features['top_features']\n","\n","    feature_cols = [] \n","    for feature in top_features: feature_cols += list(cluster_details[feature].keys())\n","\n","    feature_cols += remainder_cols\n","    \n","    rftraindf, rfvaldf, rftestdf = ftraindf.copy()[feature_cols], fvaldf.copy()[feature_cols], ftestdf.copy()[feature_cols]\n","\n","    featureSelection = {}\n","    featureSelection['top_features'] = top_features \n","    if chosen_features == None: featureSelection['mces_df'] = mces_df \n","    else: featureSelection['top_features'] = chosen_features['mces_df']\n","\n","    print(f'[MCES] Features Selected: {top_features}')\n","\n","    return rftraindf, rfvaldf, rftestdf, featureSelection\n","\n","\n","def ensemble(val_df, test_df, ref, pred_period = PRED_PERIOD):\n","    output_results = test_df[[col for col in test_df.columns if 'mdate_ref' in col or 'close' in col]]\n","    # print(output_results.columns)\n","    traincols = [col for col in val_df.columns if 'mdate_ref' not in col]\n","    traindata = val_df[traincols]\n","\n","    testcols = [col for col in test_df.columns if 'mdate_ref' not in col]\n","    testdata = test_df[testcols]\n","\n","    # print(f'traindata: {traincols}')\n","    # print(f'testdata: {testcols}')\n","\n","    headers = []\n","    r2_scores = []\n","    rmse_scores = []\n","    mape_scores = []\n","\n","    for index in range(1, pred_period + 1):\n","\n","        # train test preparation\n","        X_train = traindata.iloc[:, index :]\n","\n","        y_train = val_df.iloc[:, [0]]\n","\n","        X_test = testdata.iloc[:, index :]\n","        y_test = testdata.iloc[:, [0]]\n","\n","        model = LinearRegression()\n","        model.fit(X_train, y_train)\n","        pred = model.predict(X_test)\n","\n","        r2 = r2_score(y_test['close'], pred)\n","        rmse = mean_squared_error(y_test['close'], pred)**0.5\n","        mape = mean_absolute_percentage_error(y_test['close'], pred)\n","\n","        print(f'[TEST OUTCOME_Tp{index}] - R2: {r2}, RMSE: {rmse}, MAPE: {mape}')\n","\n","        headers.append(f'Tp{index}_pred')\n","        r2_scores.append(r2)\n","        rmse_scores.append(rmse)\n","        mape_scores.append(mape)\n","\n","        output_results[f'Tp{index}_pred'] = pred\n","\n","    stats_data = {\n","        'columns': headers,\n","        'r2': r2_scores,\n","        'rmse': rmse_scores,\n","        'mape': mape_scores\n","    }\n","    stats_df = pd.DataFrame(stats_data)\n","\n","\n","    output = ref\n","\n","    for pred_day in range(1, pred_period + 1):\n","\n","        header_ref = f'Tp{pred_day}_'\n","\n","        instance = output_results[[col for col in output_results.columns if header_ref in col]]\n","        # print(instance.columns)\n","        instance = instance.reset_index(drop = False) # pred_date out\n","        instance = instance.rename(columns = {'pred_date': f'Tp{pred_day}_date_ref', f'Tp{pred_day}_mdate_ref':'Date'})\n","        instance = instance.set_index('Date')\n","\n","        output = pd.concat([output, instance], axis = 1)\n","\n","    output = output.dropna()\n","    output1_dates = output[[col for col in output.columns if 'mdate' in col]]\n","    output1_values = output[[col for col in output.columns if 'mdate' not in col]]\n","\n","    return output_results, output1_values, output1_dates, stats_df\n","\n","# def rangeNeuralPipeline(ticker, ftraindf, fvaldf, ftestdf, cluster_details, model_dict, chosen_features = None, google = 1, pred_period = PRED_PERIOD, n_elements = 6):\n","#     if chosen_features == None:\n","#         if google == 1:\n","#             wkdir = f'/home/priya/Desktop/fyp/Src alwin/Src/s3_crystalball outcome/{ticker}/data'\n","#             with open(f'{wkdir}/mces/features_selected.pkl', 'rb') as handle:\n","#                 chosen_features = pickle.load(handle)\n","#         else:\n","#             wkdir = f'data/{ticker}/transformer'\n","#             with open(f'data/{ticker}/mces/features_selected.pkl', 'rb') as handle:\n","#                 chosen_features = pickle.load(handle)\n","\n","#     headers_compile = []\n","#     r2_compile = []\n","#     rmse_compile = []\n","#     mape_compile = []\n","\n","#     date_cols = [col for col in ftestdf.columns if '_Date' in col]\n","\n","#     for plus_target in tqdm(range(1, pred_period + 1)):\n","\n","#         # train model, predict & evaluate model performance\n","#         model, eval_res = monoNeuralPipeline(ftraindf=ftraindf, fvaldf=fvaldf, ftestdf=ftestdf, model_dict=model_dict[plus_target], cluster_details=cluster_details, plus_target=plus_target, chosen_features=chosen_features[plus_target])\n","#         # model, eval_res = monoNeuralPipeline(ftraindf=ftraindf, fvaldf=fvaldf, ftestdf=ftestdf, model_dict=model_dict, cluster_details=cluster_details, plus_target=plus_target, chosen_features=chosen_features)\n","\n","\n","#         # evaluation metrics (for summary)\n","#         iheader = eval_res['header']\n","#         ir2 = eval_res['test_pred']['r2']\n","#         irmse = eval_res['test_pred']['rmse']\n","#         imape = eval_res['test_pred']['mape']\n","\n","#         headers_compile.append(iheader)\n","#         r2_compile.append(ir2)\n","#         rmse_compile.append(irmse)\n","#         mape_compile.append(imape)\n","\n","#         # save model\n","#         # torch.save(model.state_dict(), f'{wkdir}/Tp{plus_target}.pt')\n","# # save model\n","#         if model_dict[plus_target]['model_type'] == 'transformer':\n","#             torch.save(model.state_dict(), f'{wkdir}/Tp{plus_target}.pt')\n","#             print(f\"‚úÖ PyTorch model for Tp{plus_target} saved with state_dict()\")\n","#         else:\n","#             model.save(f'{wkdir}/Tp{plus_target}_keras_model.h5')  # Save full Keras model\n","#             print(f\"‚úÖ Keras model for Tp{plus_target} saved as .h5\")\n","\n","#         # save prediction dataset\n","#         # obtain the columns for reference\n","#         instance_references = [col for col in date_cols if f'Tp{plus_target}_' in col] + ['yref_Tm0_close']\n","\n","#         # to include predicted dates by models\n","#         date_testref = ftestdf[instance_references]\n","#         date_valref = fvaldf[instance_references]\n","\n","#         # concat prediction results (to obtain date references )\n","#         test_results = pd.concat([eval_res['test_pred']['ref'], date_testref], axis = 1)\n","#         test_results = test_results.rename(columns = {'price_pred' : f'Tp{plus_target}_pred', 'yref_Tm0_close' : 'close', f'yref_Tp{plus_target}_Date': f'Tp{plus_target}_date_ref'})\n","\n","#         val_results = pd.concat([eval_res['val_pred']['ref'], date_valref], axis = 1)\n","#         val_results = val_results.rename(columns = {'price_pred' : f'Tp{plus_target}_pred', 'yref_Tm0_close' : 'close', f'yref_Tp{plus_target}_Date': f'Tp{plus_target}_date_ref'})\n","\n","#         # save the results\n","#         val_results.to_csv(f'{wkdir}/val/Tp{plus_target}_valresults.csv')\n","#         test_results.to_csv(f'{wkdir}/test/Tp{plus_target}_testresults.csv')\n","\n","#         # save train predictions for fuzzy logic\n","#         eval_res['train_pred']['cluster_ref'].to_csv(f'{wkdir}/train/Tp{plus_target}_train_clustermembership.csv')\n","#         eval_res['val_pred']['cluster_ref'].to_csv(f'{wkdir}/val/Tp{plus_target}_val_clustermembership.csv')\n","#         eval_res['test_pred']['cluster_ref'].to_csv(f'{wkdir}/test/Tp{plus_target}_test_clustermembership.csv')\n","#         print ('saved!')\n","\n","#         if plus_target == 1:\n","\n","#             for index in range(2):\n","\n","#                 if index == 1:\n","#                     item = test_results\n","#                     header = 'test'\n","#                 elif index == 0:\n","#                     item = val_results\n","#                     header = 'val'\n","\n","\n","#                 # predictions by model dates\n","#                 overall_results_predmodel = item[['close', f'Tp{plus_target}_pred', f'Tp{plus_target}_date_ref']] # by prediction model\n","#                 overall_results_predmodel.to_csv(f'{wkdir}/{header}/OVERALL_prediction_by_model.csv')\n","\n","#                 # predictions by prediction dates\n","#                 if index == 1: overall_results_predday = pd.concat([val_results, test_results], axis = 0)\n","#                 else: overall_results_predday = val_results\n","#                 close_ref = overall_results_predday[['close']]\n","#                 overall_results_predday = overall_results_predday[[f'Tp{plus_target}_date_ref', f'Tp{plus_target}_pred']]\n","\n","#                 # reindex to reference predicted date\n","#                 overall_results_predday = overall_results_predday.reset_index(drop = False)\n","#                 overall_results_predday = overall_results_predday.rename(columns = {'Date':f'Tp{plus_target}_mdate_ref', f'Tp{plus_target}_date_ref':'pred_date'})\n","#                 overall_results_predday = overall_results_predday.set_index('pred_date')\n","#                 overall_results_predday = pd.concat([overall_results_predday, close_ref], axis = 1)\n","\n","#                 if f'Tp{plus_target}_mdate_ref' not in overall_results_predday.columns:\n","#                     print(f\"‚ö†Ô∏è 'Tp{plus_target}_mdate_ref' missing in DataFrame. Available columns:\", overall_results_predday.columns)\n","#                     overall_results_predday[f'Tp{plus_target}_mdate_ref'] = pd.NaT  # or set to None or some default\n","\n","#                 overall_results_predday = overall_results_predday[['close', f'Tp{plus_target}_pred', f'Tp{plus_target}_mdate_ref']]\n","#                 overall_results_predday = overall_results_predday.dropna()\n","#                 overall_results_predday['pred_date'] = overall_results_predday.index\n","#                 overall_results_predday = overall_results_predday.set_index('pred_date')\n","#                 if index == 1: overall_results_predday = overall_results_predday.tail(len(test_results))\n","\n","#                 overall_results_predday.to_csv(f'{wkdir}/{header}/OVERALL_prediction_by_date.csv')\n","\n","#         else:\n","#             for index in range(2):\n","\n","#                 if index == 1:\n","#                     item = test_results\n","#                     header = 'test'\n","#                 elif index == 0:\n","#                     item = val_results\n","#                     header = 'val'\n","\n","\n","#                 # read from csv\n","#                 overall_results_predmodel = pd.read_csv(f'{wkdir}/{header}/OVERALL_prediction_by_model.csv', index_col = 'Date')\n","#                 overall_results_predday = pd.read_csv(f'{wkdir}/{header}/OVERALL_prediction_by_date.csv', index_col = 'pred_date')\n","\n","#                 # predmodel\n","#                 predmodel = item[[f'Tp{plus_target}_pred', f'Tp{plus_target}_date_ref']] # by prediction model\n","#                 overall_results_predmodel = pd.concat([overall_results_predmodel, predmodel], axis = 1)\n","\n","#                 # predday\n","#                 # predictions by prediction dates\n","#                 if index == 1: pred_day = pd.concat([val_results, test_results], axis = 0)\n","#                 else: pred_day = val_results\n","#                 pred_day = pred_day[[f'Tp{plus_target}_date_ref', f'Tp{plus_target}_pred']]\n","\n","#                 # reindex to reference predicted date\n","#                 pred_day = pred_day.reset_index(drop = False)\n","#                 pred_day = pred_day.rename(columns = {'Date':f'Tp{plus_target}_mdate_ref', f'Tp{plus_target}_date_ref':'pred_date'})\n","#                 pred_day = pred_day.set_index('pred_date')\n","#                 pred_day = pred_day[[f'Tp{plus_target}_pred', f'Tp{plus_target}_mdate_ref']]\n","#                 overall_results_predday = pd.concat([overall_results_predday, pred_day], axis = 1)\n","#                 overall_results_predday = overall_results_predday.dropna()\n","\n","#                 # write to csv\n","#                 overall_results_predmodel.to_csv(f'{wkdir}/{header}/OVERALL_prediction_by_model.csv')\n","#                 overall_results_predday.to_csv(f'{wkdir}/{header}/OVERALL_prediction_by_date.csv')\n","\n","#         gc.collect()\n","\n","#     ref = overall_results_predmodel[['close']]\n","\n","#     stats_data = {\n","#         'columns': headers_compile,\n","#         'r2_price': r2_compile,\n","#         'rmse': rmse_compile,\n","#         'mape': mape_compile\n","#     }\n","#     stats_df = pd.DataFrame(stats_data)\n","#     stats_df.to_csv(f'{wkdir}/statistics_before_ensemble.csv')\n","\n","#     return overall_results_predmodel, overall_results_predday\n","\n"]},{"cell_type":"code","execution_count":57,"id":"84484c6b","metadata":{},"outputs":[],"source":["# üì¶ Required imports\n","import pandas as pd\n","import torch\n","import gc\n","from tqdm.auto import tqdm\n","import pickle\n","\n","# ‚úÖ Helper Function: Update Overall Prediction Files\n","def update_overall_predictions(plus_target, val_results, test_results, wkdir, header, date_cols):\n","    item = test_results if header == 'test' else val_results\n","\n","    pred_model_path = f\"{wkdir}/{header}/OVERALL_prediction_by_modelmlp.csv\"\n","    pred_day_path = f\"{wkdir}/{header}/OVERALL_prediction_by_datemlp.csv\"\n","\n","    if plus_target == 1:\n","        # Save prediction-by-model\n","        overall_results_predmodel = item[['close', f'Tp{plus_target}_pred', f'Tp{plus_target}_date_ref']]\n","        overall_results_predmodel.to_csv(pred_model_path)\n","\n","        # Combine data for prediction-by-date\n","        overall_results_predday = pd.concat([val_results, test_results], axis=0) if header == 'test' else val_results\n","        close_ref = overall_results_predday[['close']]\n","\n","        overall_results_predday = overall_results_predday[[f'Tp{plus_target}_date_ref', f'Tp{plus_target}_pred']]\n","        overall_results_predday = overall_results_predday.reset_index(drop=False)\n","        overall_results_predday = overall_results_predday.rename(columns={\n","            'Date': f'Tp{plus_target}_mdate_ref',\n","            f'Tp{plus_target}_date_ref': 'pred_date'\n","        })\n","\n","        # Remove any duplicated pred_date before setting it as index\n","        overall_results_predday = overall_results_predday.drop_duplicates(subset='pred_date', keep='first')\n","        close_ref = close_ref[~close_ref.index.duplicated(keep='first')]\n","\n","        # Set index and align\n","        overall_results_predday = overall_results_predday.set_index('pred_date')\n","        shared_index = overall_results_predday.index.intersection(close_ref.index)\n","        overall_results_predday = overall_results_predday.loc[shared_index]\n","        close_ref = close_ref.loc[shared_index]\n","\n","        # Merge\n","        overall_results_predday = pd.concat([overall_results_predday, close_ref], axis=1)\n","\n","        if f'Tp{plus_target}_mdate_ref' not in overall_results_predday.columns:\n","            overall_results_predday[f'Tp{plus_target}_mdate_ref'] = pd.NaT\n","\n","        overall_results_predday = overall_results_predday[['close', f'Tp{plus_target}_pred', f'Tp{plus_target}_mdate_ref']]\n","        overall_results_predday = overall_results_predday.dropna()\n","        overall_results_predday['pred_date'] = overall_results_predday.index\n","        overall_results_predday = overall_results_predday.set_index('pred_date')\n","\n","        if header == 'test':\n","            overall_results_predday = overall_results_predday.tail(len(test_results))\n","\n","        overall_results_predday.to_csv(pred_day_path)\n","\n","    else:\n","        # Load existing CSVs\n","        df_temp = pd.read_csv(pred_model_path)\n","        if 'Date' in df_temp.columns:\n","            df_temp.set_index('Date', inplace=True)\n","        overall_results_predmodel = df_temp\n","\n","        overall_results_predday = pd.read_csv(pred_day_path, index_col='pred_date')\n","\n","        # Update prediction-by-model\n","        predmodel = item[[f'Tp{plus_target}_pred', f'Tp{plus_target}_date_ref']]\n","        overall_results_predmodel = pd.concat([overall_results_predmodel, predmodel], axis=1)\n","\n","        # Update prediction-by-date\n","        pred_day = pd.concat([val_results, test_results], axis=0) if header == 'test' else val_results\n","        pred_day = pred_day[[f'Tp{plus_target}_date_ref', f'Tp{plus_target}_pred']]\n","        pred_day = pred_day.reset_index(drop=False)\n","        pred_day = pred_day.rename(columns={\n","            'Date': f'Tp{plus_target}_mdate_ref',\n","            f'Tp{plus_target}_date_ref': 'pred_date'\n","        })\n","        pred_day = pred_day.set_index('pred_date')\n","        pred_day = pred_day[[f'Tp{plus_target}_pred', f'Tp{plus_target}_mdate_ref']]\n","\n","        overall_results_predday = pd.concat([overall_results_predday, pred_day], axis=1)\n","        overall_results_predday = overall_results_predday.dropna()\n","\n","        # Save both\n","        overall_results_predmodel.to_csv(pred_model_path)\n","        overall_results_predday.to_csv(pred_day_path)\n","\n","    return overall_results_predmodel, overall_results_predday\n","\n","# üöÄ Main Function\n","\n","def rangeNeuralPipeline(ticker, ftraindf, fvaldf, ftestdf, cluster_details, model_dict, chosen_features=None, google=1, pred_period=13, n_elements=6):\n","    if chosen_features is None:\n","        wkdir = f'/home/priya/Desktop/fyp/Src alwin/Src/data/{ticker}'\n","        with open(f'{wkdir}/features_selected.pkl', 'rb') as handle:\n","            chosen_features = pickle.load(handle)\n","\n","    headers_compile, r2_compile, rmse_compile, mape_compile = [], [], [], []\n","    date_cols = [col for col in ftestdf.columns if '_Date' in col]\n","\n","    for plus_target in tqdm(range(1, pred_period + 1)):\n","        model, eval_res = monoNeuralPipeline(\n","            ftraindf=ftraindf,\n","            fvaldf=fvaldf,\n","            ftestdf=ftestdf,\n","            model_dict=model_dict[plus_target],\n","            cluster_details=cluster_details,\n","            plus_target=plus_target,\n","            chosen_features=chosen_features[plus_target]\n","        )\n","\n","        headers_compile.append(eval_res['header'])\n","        r2_compile.append(eval_res['test_pred']['r2'])\n","        rmse_compile.append(eval_res['test_pred']['rmse'])\n","        mape_compile.append(eval_res['test_pred']['mape'])\n","\n","        # Save model\n","        if model_dict[plus_target]['model_type'] == 'transformer':\n","            torch.save(model.state_dict(), f'{wkdir}/Tp{plus_target}.pt')\n","        else:\n","            model.save(f'{wkdir}/Tp{plus_target}_keras_model.h5')\n","\n","\n","        # Save predictions with reference\n","        date_testref = ftestdf[[col for col in date_cols if f'Tp{plus_target}_' in col] + ['yref_Tm0_close']]\n","        date_valref = fvaldf[[col for col in date_cols if f'Tp{plus_target}_' in col] + ['yref_Tm0_close']]\n","\n","        test_results = pd.concat([eval_res['test_pred']['ref'], date_testref], axis=1)\n","        test_results = test_results.rename(columns={\n","            'price_pred': f'Tp{plus_target}_pred',\n","            'yref_Tm0_close': 'close',\n","            f'yref_Tp{plus_target}_Date': f'Tp{plus_target}_date_ref'\n","        })\n","\n","        val_results = pd.concat([eval_res['val_pred']['ref'], date_valref], axis=1)\n","        val_results = val_results.rename(columns={\n","            'price_pred': f'Tp{plus_target}_pred',\n","            'yref_Tm0_close': 'close',\n","            f'yref_Tp{plus_target}_Date': f'Tp{plus_target}_date_ref'\n","        })\n","        import os\n","\n","        # Ensure subdirectories exist\n","        os.makedirs(f'{wkdir}/train', exist_ok=True)\n","        os.makedirs(f'{wkdir}/val', exist_ok=True)\n","        os.makedirs(f'{wkdir}/test', exist_ok=True)\n","\n","        val_results.to_csv(f'{wkdir}/val/Tp{plus_target}_valresults.csv')\n","        test_results.to_csv(f'{wkdir}/test/Tp{plus_target}_testresults.csv')\n","\n","        # Save cluster membership (train/val/test)\n","        if 'train_pred' in eval_res:\n","            eval_res['train_pred']['cluster_ref'].to_csv(f'{wkdir}/train/Tp{plus_target}_train_clustermembership.csv')\n","        else:\n","            print(f\"‚ö†Ô∏è 'train_pred' missing for Tp{plus_target}\")\n","\n","        eval_res['val_pred']['cluster_ref'].to_csv(f'{wkdir}/val/Tp{plus_target}_val_clustermembership.csv')\n","        eval_res['test_pred']['cluster_ref'].to_csv(f'{wkdir}/test/Tp{plus_target}_test_clustermembership.csv')\n","\n","        # for index in range(2):\n","        #     header = 'test' if index == 1 else 'val'\n","        #     overall_results_predmodel, overall_results_predday = update_overall_predictions(\n","        #         plus_target, val_results, test_results, wkdir, header, date_cols\n","        #     )\n","\n","        gc.collect()\n","\n","    # Save summary statistics\n","    stats_df = pd.DataFrame({\n","        'columns': headers_compile,\n","        'r2_price': r2_compile,\n","        'rmse': rmse_compile,\n","        'mape': mape_compile\n","    })\n","    stats_df.to_csv(f'{wkdir}/mlp_statistics_before_ensemble.csv')\n","\n","    return overall_results_predmodel, overall_results_predday\n","    # return \n"]},{"cell_type":"code","execution_count":58,"id":"9667bb19","metadata":{},"outputs":[],"source":["import random, pickle\n","import pandas as pd \n","from tqdm.auto import tqdm\n","from datetime import datetime\n","\n","import os, sys \n","\n","from sklearn.metrics import mean_squared_error\n","from s1_data_preparation.config import *\n","from s2_crystal_ball.config import *\n","\n","def mcesDefuzzy(m1_pred, header, y_cols, cluster_details): \n","\n","    m1_pred['minimum'] = m1_pred.apply(lambda x: min(x), axis = 1)\n","    for col in y_cols: m1_pred[col] = m1_pred.apply(lambda x: round((x[col] - x['minimum']), 6), axis = 1)\n","\n","    m1_pred['summation'] = m1_pred.apply(lambda x: sum(x), axis = 1)\n","    for col in y_cols: m1_pred[col] = m1_pred.apply(lambda x: round(x[col]/x['summation'], 6), axis = 1)\n","\n","    m1_pred = m1_pred.drop(['minimum', 'summation'], axis = 1) \n","\n","    m1_pred['pc_pred'] = 0 \n","    for col in y_cols: m1_pred['pc_pred'] = m1_pred.apply(lambda x: x['pc_pred'] + x[col]*cluster_details[header][col]['mean'], axis = 1)\n","\n","    return m1_pred['pc_pred']\n","\n","def mces(ftraindf, fvaldf, ftestdf, cluster_details, plus_target, chosen_features = None, threshold = 10, iterations = 0): \n","\n","    print(f'[mces initiated - {plus_target}] training mlp model ...')\n","\n","    # price_features = [col for col in list(cluster_details.keys()) if 'Price' in col and 'x_' in col]\n","    x_cols = [col for col in ftraindf.columns if 'x_' in col]\n","    remainder_cols = [col for col in ftraindf.columns if col not in x_cols]\n","\n","    if chosen_features == None: \n","\n","        # define MLP Model \n","        model_dict = {\n","            'model_type': 'mlp', \n","            'mlp': {\n","                'layers': {\n","                    0: {'nodes': 256}, \n","                    1: {'nodes': 64}, \n","                    2: {'nodes': 128}, \n","                }, \n","                'hl_activation': 'relu', \n","                'ol_activation': 'sigmoid',\n","                'optimizer': {\n","                    'optim_type': 'adam', \n","                    'learning_rate': 0.0001,\n","                }, \n","                'shuffle': False, \n","                'verbose': 0,\n","            },\n","            'early_stopper': {\n","                'patience': 5, \n","                'min_delta': 0        \n","            },\n","            'epochs': 20,\n","            'batch_size': 16,\n","        }\n","\n","        # train a model with full features \n","        model, eval_res = monoNeuralPipeline(ftraindf=ftraindf, fvaldf=fvaldf, ftestdf=ftestdf, model_dict=model_dict, cluster_details=cluster_details, plus_target=plus_target, mode = 1)\n","\n","        # x_features = [col for col in list(cluster_details.keys()) if 'x_' in col and col not in price_features]\n","        x_features = [col for col in list(cluster_details.keys()) if 'x_' in col]        \n","        y_cols = [col for col in ftraindf.columns if f'y_Tp{plus_target}_' in col]\n","        header = f'y_Tp{plus_target}_PriceChg'\n","        target = ftraindf[f'ypcref_Tp{plus_target}_PriceChg']\n","\n","        X_mces = ftraindf[x_cols].reset_index(drop = True)\n","\n","        mces_df = pd.DataFrame({'cols': x_features})\n","        scores = [0 for item in range(len(x_features))] \n","        frequency = [0 for item in range(len(x_features))] \n","        in_mask = [0 for item in range(len(x_features))] \n","        \n","        if iterations == 0: iterations = len(X_mces.index)\n","        # print(f'xcols len: {len(x_cols)}')\n","        for row_index in tqdm(range(iterations)): \n","\n","            temp = X_mces.copy()[X_mces.index == row_index]\n","            temp_target = pd.Series(target[row_index])\n","\n","            mask1 = [random.randint(0, 1) for col in range(len(x_features))]\n","\n","            # in case no more features left \n","            while sum(mask1) < 7: mask1 = [random.randint(0, 1) for col in range(len(x_features))]\n","            mces_df['mask1'] = mask1\n","\n","            disabled = list(mces_df[mces_df['mask1'] == 0]['cols'])\n","            enabled = list(mces_df[mces_df['mask1'] == 1]['cols'])\n","\n","            inverted = random.choice(enabled)\n","            inverted_index = x_features.index(inverted)\n","\n","            # retrieve respective cols \n","\n","            for col in disabled: \n","                for cluster in list(cluster_details[col].keys()): \n","                    temp[cluster] = 0\n","            for col in enabled: in_mask[x_features.index(col)] += 1\n","            frequency[inverted_index] += 1\n","\n","            # mask 1\n","            pred = model.predict(temp, verbose = 0)\n","\n","            m1_pred = pd.DataFrame(pred, columns=y_cols)\n","            m1_predp = mcesDefuzzy(m1_pred, header, y_cols, cluster_details)\n","            rmse1 = mean_squared_error(temp_target, m1_predp)\n","\n","            for cluster in list(cluster_details[inverted].keys()): \n","                temp[cluster] = X_mces[cluster].mean()\n","\n","            m2_pred = pd.DataFrame(model.predict(temp, verbose = model_dict['mlp']['verbose']), columns=y_cols)\n","            m2_predp = mcesDefuzzy(m2_pred, header, y_cols, cluster_details)\n","            rmse2 = mean_squared_error(temp_target, m2_predp)\n","\n","            rmseDiff = rmse2 - rmse1\n","\n","            scores[inverted_index] += rmseDiff\n","            \n","            # print(row_index)\n","            # if row_index > 10: break\n","\n","\n","        mces_df['scores'] = scores\n","        mces_df['in_mask_freq'] = in_mask\n","        mces_df['frequency'] = frequency\n","        mces_df = mces_df.drop(['mask1'], axis = 1)\n","        \n","        try: \n","            mces_df['weighted_scores'] = mces_df.apply(lambda x: x['scores']/x['frequency'], axis = 1)\n","        except: \n","            mces_df['weighted_scores'] = mces_df.apply(lambda x: x['scores'], axis = 1)\n","        mces_df = mces_df.sort_values(by = ['weighted_scores'], ascending = False)    \n","        \n","        top_features = list(mces_df[mces_df['weighted_scores'] > 0]['cols'])\n","#         if len(top_features) < threshold: top_features = list(mces_df.head(threshold)['cols'])\n","        \n","        # top_features += price_features\n","    \n","    else: top_features = chosen_features['top_features']\n","\n","    feature_cols = [] \n","    for feature in top_features: feature_cols += list(cluster_details[feature].keys())\n","\n","    feature_cols += remainder_cols\n","    \n","    # featureSelection = {}\n","    # featureSelection['top_features'] = top_features \n","    # if chosen_features == None: featureSelection['mces_df'] = mces_df \n","    # else: featureSelection['top_features'] = chosen_features['mces_df']\n","\n","    print(f'[MCES] Features Selected: {top_features}')\n","\n","    return feature_cols, mces_df \n","\n","\n","def mcesPipeline(ticker, ftraindf, fvaldf, ftestdf, cluster_details, start_index=1, end_index=13, pred_period=13): \n","    print(f\"üìå Starting MCES Pipeline for {ticker} from Tp{start_index} to Tp{end_index}\")\n","\n","    # Load or initialize features_selected dictionary\n","    try:\n","        with open(f'/home/priya/Desktop/fyp/Src alwin/Src/s3_crystalball outcome/{ticker}/data/mces/features_selected.pkl', 'rb') as handle:\n","            features_selected = pickle.load(handle)   \n","        print(\"‚úÖ Loaded existing features_selected.pkl\")\n","    except FileNotFoundError: \n","        features_selected = {}\n","        with open(f'/home/priya/Desktop/fyp/Src alwin/Src/s3_crystalball outcome/{ticker}/data/mces/features_selected.pkl', 'wb') as fp: \n","            pickle.dump(features_selected, fp)\n","        print(\"‚ö†Ô∏è features_selected.pkl not found. Initialized empty dictionary.\")\n","\n","\n","        # Ensure 'Date' is in datetime format\n","        print(\"üïí Checking and converting 'Date' column to datetime format if needed...\")\n","\n","        if isinstance(ftraindf['Date'].iloc[0], str):  # Convert if it's a string\n","            ftraindf['Date'] = pd.to_datetime(ftraindf['Date']).dt.date\n","            print(\"‚úÖ Converted 'Date' from string to datetime.date format.\")\n","        elif isinstance(ftraindf['Date'].iloc[0], pd.Timestamp):  # Convert if it's datetime\n","            ftraindf['Date'] = ftraindf['Date'].dt.date\n","            print(\"‚úÖ Converted 'Date' from datetime to datetime.date format.\")\n","        else:\n","            print(\"‚ö†Ô∏è 'Date' is already in correct datetime.date format. No conversion needed.\")\n","\n","        # Now apply the MCES date filter\n","        print(f\"üîç Filtering dataset between {MCES_START} and {MCES_END}...\")\n","        ftraindf = ftraindf[(ftraindf['Date'] >= MCES_START) & (ftraindf['Date'] < MCES_END)]\n","        print(f\"‚úÖ Filtered dataset shape: {ftraindf.shape}\")\n","\n","    # Loop through prediction targets\n","    for plus_target in tqdm(range(start_index, end_index + 1)): \n","        print(f\"\\nüöÄ Running MCES for Tp{plus_target}...\")\n","\n","        # Run MCES feature selection\n","        try:\n","            feature_cols, mces_df = mces(ftraindf, fvaldf, ftestdf, cluster_details, plus_target)\n","            print(f\"‚úÖ MCES completed for Tp{plus_target}. Selected {len(feature_cols)} features.\")\n","        except Exception as e:\n","            print(f\"‚ùå Error in MCES for Tp{plus_target}: {e}\")\n","            continue\n","\n","        # Load existing feature selection results\n","        try:\n","            with open(f'/home/priya/Desktop/fyp/Src alwin/Src/s3_crystalball outcome/{ticker}/data/mces/features_selected.pkl', 'rb') as handle:\n","                features_selected = pickle.load(handle)    \n","        except FileNotFoundError:\n","            print(f\"‚ö†Ô∏è features_selected.pkl missing while loading for Tp{plus_target}. Initializing new dict.\")\n","            features_selected = {}\n","\n","        # Store selected features\n","        features_selected[plus_target] = feature_cols\n","        print(f\"üíæ Saving selected features for Tp{plus_target}...\")\n","\n","        # Save MCES DataFrame\n","        mces_file_path = f'/home/priya/Desktop/fyp/Src alwin/Src/s3_crystalball outcome/{ticker}/data/mces/Tp{plus_target}_mcesdf.csv'\n","        try:\n","            mces_df.to_csv(mces_file_path, index=False)\n","            print(f\"‚úÖ MCES DataFrame saved at: {mces_file_path}\")\n","        except Exception as e:\n","            print(f\"‚ùå Error saving MCES DataFrame for Tp{plus_target}: {e}\")\n","\n","        # Save updated features_selected\n","        try:\n","            with open(f'/home/priya/Desktop/fyp/Src alwin/Src/s3_crystalball outcome/{ticker}/data/mces/features_selected.pkl', 'wb') as fp: \n","                pickle.dump(features_selected, fp)  \n","            print(f\"‚úÖ Updated features_selected.pkl with Tp{plus_target} features.\")\n","        except Exception as e:\n","            print(f\"‚ùå Error saving features_selected.pkl for Tp{plus_target}: {e}\")\n","\n","    print(\"\\nüéØ MCES Pipeline completed successfully!\\n\")\n","    return features_selected  \n"]},{"cell_type":"code","execution_count":59,"id":"355e5089","metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Transformer(nn.Module):\n","    def __init__(self, model_dict):\n","        super().__init__()\n","\n","        input_size = model_dict['input_size']\n","        output_size = model_dict['output_size']\n","        config = model_dict['transformer']\n","\n","        d_model = config.get('d_model', input_size)  # usually should match input_size or be embedded\n","        nhead = config.get('nhead', 4)\n","        dim_feedforward = config.get('dim_feedforward', 256)\n","        num_layers = config.get('num_encoder_layers', 2)\n","        dropout = config.get('dropout', 0.1)\n","\n","        self.encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","\n","        # Dense layer settings\n","        self.hidden_size = config.get('hidden_size', 128)\n","        self.dense_layers = config.get('dense_layers', 2)\n","\n","        self.activation = nn.ReLU()\n","        self.output_activation = nn.Identity() if model_dict.get(\"loss_fn\", \"\") != \"BCELoss\" else nn.Sigmoid()\n","\n","        self.ll1 = nn.Linear(d_model, self.hidden_size)\n","        self.ll2 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.ll3 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.ll4 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.ll5 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.ll6 = nn.Linear(self.hidden_size, output_size)\n","\n","        # Dense layer shortcut config\n","        if self.dense_layers == 1:\n","            self.ll1 = nn.Linear(d_model, output_size)\n","        elif self.dense_layers == 2:\n","            self.ll2 = nn.Linear(self.hidden_size, output_size)\n","        elif self.dense_layers == 3:\n","            self.ll3 = nn.Linear(self.hidden_size, output_size)\n","        elif self.dense_layers == 4:\n","            self.ll4 = nn.Linear(self.hidden_size, output_size)\n","        elif self.dense_layers == 5:\n","            self.ll5 = nn.Linear(self.hidden_size, output_size)\n","        elif self.dense_layers >= 6:\n","            self.ll6 = nn.Linear(self.hidden_size, output_size)\n","\n","    def forward(self, x):\n","        x = self.transformer_encoder(x)\n","        x = self.ll1(x)\n","\n","        if self.dense_layers >= 2:\n","            x = self.activation(x)\n","            x = self.ll2(x)\n","\n","        if self.dense_layers >= 3:\n","            x = self.activation(x)\n","            x = self.ll3(x)\n","\n","        if self.dense_layers >= 4:\n","            x = self.activation(x)\n","            x = self.ll4(x)\n","\n","        if self.dense_layers >= 5:\n","            x = self.activation(x)\n","            x = self.ll5(x)\n","\n","        if self.dense_layers >= 6:\n","            x = self.activation(x)\n","            x = self.ll6(x)\n","\n","        x = self.output_activation(x)\n","        return x\n"]},{"cell_type":"code","execution_count":60,"id":"5cf36354","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec5eca2b038c4b1c91f43d3c007e7d1a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/13 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.148, R2=0.996, MAPE=1.326%\n","val_pred: RMSE=0.429, R2=0.841, MAPE=1.918%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["test_pred: RMSE=0.507, R2=0.988, MAPE=1.820%\n"]},{"name":"stderr","output_type":"stream","text":["/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.160, R2=0.996, MAPE=1.469%\n","val_pred: RMSE=0.513, R2=0.774, MAPE=2.405%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["test_pred: RMSE=0.587, R2=0.985, MAPE=2.109%\n"]},{"name":"stderr","output_type":"stream","text":["/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.192, R2=0.994, MAPE=1.708%\n","val_pred: RMSE=0.552, R2=0.741, MAPE=2.544%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["test_pred: RMSE=0.697, R2=0.978, MAPE=2.421%\n"]},{"name":"stderr","output_type":"stream","text":["/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.213, R2=0.992, MAPE=2.094%\n","val_pred: RMSE=0.685, R2=0.606, MAPE=3.193%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["test_pred: RMSE=0.808, R2=0.971, MAPE=2.868%\n"]},{"name":"stderr","output_type":"stream","text":["/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.267, R2=0.988, MAPE=2.590%\n","val_pred: RMSE=0.793, R2=0.474, MAPE=3.817%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["test_pred: RMSE=0.977, R2=0.957, MAPE=3.418%\n"]},{"name":"stderr","output_type":"stream","text":["/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.272, R2=0.988, MAPE=2.565%\n","val_pred: RMSE=0.814, R2=0.450, MAPE=3.875%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["test_pred: RMSE=0.987, R2=0.956, MAPE=3.546%\n"]},{"name":"stderr","output_type":"stream","text":["/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.285, R2=0.986, MAPE=2.591%\n","val_pred: RMSE=0.808, R2=0.459, MAPE=3.742%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["test_pred: RMSE=1.091, R2=0.947, MAPE=3.910%\n"]},{"name":"stderr","output_type":"stream","text":["/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.303, R2=0.985, MAPE=2.781%\n","val_pred: RMSE=0.882, R2=0.357, MAPE=4.119%\n","test_pred: RMSE=1.113, R2=0.944, MAPE=3.961%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.281, R2=0.987, MAPE=2.577%\n","val_pred: RMSE=0.820, R2=0.445, MAPE=3.694%\n","test_pred: RMSE=1.055, R2=0.950, MAPE=3.681%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.291, R2=0.986, MAPE=2.598%\n","val_pred: RMSE=0.897, R2=0.337, MAPE=4.155%\n","test_pred: RMSE=1.133, R2=0.943, MAPE=4.011%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.330, R2=0.982, MAPE=2.950%\n","val_pred: RMSE=0.983, R2=0.204, MAPE=4.410%\n","test_pred: RMSE=1.172, R2=0.939, MAPE=4.137%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.480, R2=0.962, MAPE=3.646%\n","val_pred: RMSE=0.922, R2=0.300, MAPE=4.186%\n","test_pred: RMSE=1.215, R2=0.934, MAPE=4.228%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","/home/priya/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Reloaded cluster_details\n","train_pred: RMSE=0.447, R2=0.967, MAPE=3.714%\n","val_pred: RMSE=1.112, R2=-0.018, MAPE=5.165%\n","test_pred: RMSE=1.310, R2=0.923, MAPE=4.656%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"ename":"NameError","evalue":"name 'overall_results_predmodel' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# üöÄ Run Neural Pipeline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrangeNeuralPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mticker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mftraindf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mftraindf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfvaldf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvaldf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mftestdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mftestdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcluster_details\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_details\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgoogle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPRED_PERIOD\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[57], line 177\u001b[0m, in \u001b[0;36mrangeNeuralPipeline\u001b[0;34m(ticker, ftraindf, fvaldf, ftestdf, cluster_details, model_dict, chosen_features, google, pred_period, n_elements)\u001b[0m\n\u001b[1;32m    169\u001b[0m stats_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m: headers_compile,\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2_price\u001b[39m\u001b[38;5;124m'\u001b[39m: r2_compile,\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m: rmse_compile,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmape\u001b[39m\u001b[38;5;124m'\u001b[39m: mape_compile\n\u001b[1;32m    174\u001b[0m })\n\u001b[1;32m    175\u001b[0m stats_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwkdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/mlp_statistics_before_ensemble.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverall_results_predmodel\u001b[49m, overall_results_predday\n","\u001b[0;31mNameError\u001b[0m: name 'overall_results_predmodel' is not defined"]}],"source":["# üöÄ Run Neural Pipeline\n","rangeNeuralPipeline(\n","    ticker=ticker,\n","    ftraindf=ftraindf,\n","    fvaldf=fvaldf,\n","    ftestdf=ftestdf,\n","    cluster_details=cluster_details,\n","    model_dict=model_dict,\n","    google=1,\n","    pred_period=PRED_PERIOD\n",")"]},{"cell_type":"code","execution_count":13,"id":"12503a17","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{1: {'batch_size': 16,\n","     'day_target': 1,\n","     'early_stopper': {'min_delta': 0, 'patience': 5},\n","     'epochs': 20,\n","     'input_size': 273,\n","     'mlp': {'hl_activation': 'relu',\n","             'layers': {0: {'nodes': 256}, 1: {'nodes': 64}, 2: {'nodes': 128}},\n","             'ol_activation': 'sigmoid',\n","             'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","             'shuffle': False,\n","             'verbose': 0},\n","     'model_type': 'mlp',\n","     'output_size': 7},\n"," 2: {'batch_size': 16,\n","     'day_target': 2,\n","     'early_stopper': {'min_delta': 0, 'patience': 5},\n","     'epochs': 20,\n","     'input_size': 273,\n","     'mlp': {'hl_activation': 'relu',\n","             'layers': {0: {'nodes': 256}, 1: {'nodes': 64}, 2: {'nodes': 128}},\n","             'ol_activation': 'sigmoid',\n","             'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","             'shuffle': False,\n","             'verbose': 0},\n","     'model_type': 'mlp',\n","     'output_size': 7},\n"," 3: {'batch_size': 16,\n","     'day_target': 3,\n","     'early_stopper': {'min_delta': 0, 'patience': 5},\n","     'epochs': 20,\n","     'input_size': 273,\n","     'mlp': {'hl_activation': 'relu',\n","             'layers': {0: {'nodes': 256}, 1: {'nodes': 64}, 2: {'nodes': 128}},\n","             'ol_activation': 'sigmoid',\n","             'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","             'shuffle': False,\n","             'verbose': 0},\n","     'model_type': 'mlp',\n","     'output_size': 7},\n"," 4: {'batch_size': 16,\n","     'day_target': 4,\n","     'early_stopper': {'min_delta': 0, 'patience': 5},\n","     'epochs': 20,\n","     'input_size': 273,\n","     'mlp': {'hl_activation': 'relu',\n","             'layers': {0: {'nodes': 256}, 1: {'nodes': 64}, 2: {'nodes': 128}},\n","             'ol_activation': 'sigmoid',\n","             'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","             'shuffle': False,\n","             'verbose': 0},\n","     'model_type': 'mlp',\n","     'output_size': 7},\n"," 5: {'batch_size': 16,\n","     'day_target': 5,\n","     'early_stopper': {'min_delta': 0, 'patience': 5},\n","     'epochs': 20,\n","     'input_size': 273,\n","     'mlp': {'hl_activation': 'relu',\n","             'layers': {0: {'nodes': 256}, 1: {'nodes': 64}, 2: {'nodes': 128}},\n","             'ol_activation': 'sigmoid',\n","             'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","             'shuffle': False,\n","             'verbose': 0},\n","     'model_type': 'mlp',\n","     'output_size': 7},\n"," 6: {'batch_size': 16,\n","     'day_target': 6,\n","     'early_stopper': {'min_delta': 0, 'patience': 5},\n","     'epochs': 20,\n","     'input_size': 273,\n","     'mlp': {'hl_activation': 'relu',\n","             'layers': {0: {'nodes': 256}, 1: {'nodes': 64}, 2: {'nodes': 128}},\n","             'ol_activation': 'sigmoid',\n","             'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","             'shuffle': False,\n","             'verbose': 0},\n","     'model_type': 'mlp',\n","     'output_size': 7},\n"," 7: {'batch_size': 16,\n","     'day_target': 7,\n","     'early_stopper': {'min_delta': 0, 'patience': 5},\n","     'epochs': 20,\n","     'input_size': 273,\n","     'mlp': {'hl_activation': 'relu',\n","             'layers': {0: {'nodes': 256}, 1: {'nodes': 64}, 2: {'nodes': 128}},\n","             'ol_activation': 'sigmoid',\n","             'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","             'shuffle': False,\n","             'verbose': 0},\n","     'model_type': 'mlp',\n","     'output_size': 7},\n"," 8: {'batch_size': 16,\n","     'day_target': 8,\n","     'early_stopper': {'min_delta': 0, 'patience': 5},\n","     'epochs': 20,\n","     'input_size': 273,\n","     'mlp': {'hl_activation': 'relu',\n","             'layers': {0: {'nodes': 256}, 1: {'nodes': 64}, 2: {'nodes': 128}},\n","             'ol_activation': 'sigmoid',\n","             'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","             'shuffle': False,\n","             'verbose': 0},\n","     'model_type': 'mlp',\n","     'output_size': 7},\n"," 9: {'batch_size': 16,\n","     'day_target': 9,\n","     'early_stopper': {'min_delta': 0, 'patience': 5},\n","     'epochs': 20,\n","     'input_size': 273,\n","     'mlp': {'hl_activation': 'relu',\n","             'layers': {0: {'nodes': 256}, 1: {'nodes': 64}, 2: {'nodes': 128}},\n","             'ol_activation': 'sigmoid',\n","             'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","             'shuffle': False,\n","             'verbose': 0},\n","     'model_type': 'mlp',\n","     'output_size': 7},\n"," 10: {'batch_size': 16,\n","      'day_target': 10,\n","      'early_stopper': {'min_delta': 0, 'patience': 5},\n","      'epochs': 20,\n","      'input_size': 273,\n","      'mlp': {'hl_activation': 'relu',\n","              'layers': {0: {'nodes': 256},\n","                         1: {'nodes': 64},\n","                         2: {'nodes': 128}},\n","              'ol_activation': 'sigmoid',\n","              'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","              'shuffle': False,\n","              'verbose': 0},\n","      'model_type': 'mlp',\n","      'output_size': 7},\n"," 11: {'batch_size': 16,\n","      'day_target': 11,\n","      'early_stopper': {'min_delta': 0, 'patience': 5},\n","      'epochs': 20,\n","      'input_size': 273,\n","      'mlp': {'hl_activation': 'relu',\n","              'layers': {0: {'nodes': 256},\n","                         1: {'nodes': 64},\n","                         2: {'nodes': 128}},\n","              'ol_activation': 'sigmoid',\n","              'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","              'shuffle': False,\n","              'verbose': 0},\n","      'model_type': 'mlp',\n","      'output_size': 7},\n"," 12: {'batch_size': 16,\n","      'day_target': 12,\n","      'early_stopper': {'min_delta': 0, 'patience': 5},\n","      'epochs': 20,\n","      'input_size': 273,\n","      'mlp': {'hl_activation': 'relu',\n","              'layers': {0: {'nodes': 256},\n","                         1: {'nodes': 64},\n","                         2: {'nodes': 128}},\n","              'ol_activation': 'sigmoid',\n","              'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","              'shuffle': False,\n","              'verbose': 0},\n","      'model_type': 'mlp',\n","      'output_size': 7},\n"," 13: {'batch_size': 16,\n","      'day_target': 13,\n","      'early_stopper': {'min_delta': 0, 'patience': 5},\n","      'epochs': 20,\n","      'input_size': 273,\n","      'mlp': {'hl_activation': 'relu',\n","              'layers': {0: {'nodes': 256},\n","                         1: {'nodes': 64},\n","                         2: {'nodes': 128}},\n","              'ol_activation': 'sigmoid',\n","              'optimizer': {'learning_rate': 0.001, 'optim_type': 'adam'},\n","              'shuffle': False,\n","              'verbose': 0},\n","      'model_type': 'mlp',\n","      'output_size': 7}}\n"]}],"source":["import pprint\n","pprint.pprint(model_dict)\n"]},{"cell_type":"code","execution_count":15,"id":"06b115b9","metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] Unable to synchronously open file (unable to open file: name = 'model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Print all layer weights\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mlayers):\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    190\u001b[0m         filepath,\n\u001b[1;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    194\u001b[0m     )\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/legacy/saving/legacy_h5_format.py:116\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[0;32m--> 116\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     f \u001b[38;5;241m=\u001b[39m filepath\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n","File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"]}],"source":["from tensorflow.keras.models import load_model\n","\n","model = load_model(\"model.h5\")\n","\n","# Print all layer weights\n","for i, layer in enumerate(model.layers):\n","    print(f\"\\nüîπ Layer {i}: {layer.name}\")\n","    weights = layer.get_weights()\n","    if weights:\n","        for j, weight in enumerate(weights):\n","            print(f\"  Weight {j} shape: {weight.shape}\")\n","            print(weight)  # This prints the actual values ‚Äî remove if too long\n","    else:\n","        print(\"  No trainable weights in this layer.\")\n","\n","import numpy as np\n","\n","# Example: Make prediction\n","predictions = model.predict(rftestdf.drop(columns=['your_target_column']))\n","\n","# Compare predictions to ground truth\n","from sklearn.metrics import r2_score, mean_squared_error\n","\n","true = rftestdf['ypcref_Tp13_PriceChg']\n","r2 = r2_score(true, predictions)\n","rmse = mean_squared_error(true, predictions, squared=False)\n","\n","print(f\"\\nüìä R2 Score: {r2:.4f}\")\n","print(f\"üìâ RMSE: {rmse:.4f}\")"]},{"cell_type":"code","execution_count":25,"id":"70d978e6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","üîé Evaluation Metrics:\n"]},{"ename":"NameError","evalue":"name 'eval_res' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# üìä Print R¬≤ and RMSE values\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîé Evaluation Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain R¬≤ : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43meval_res\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_pred\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain RMSE : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_pred\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation R¬≤ : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_pred\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'eval_res' is not defined"]}],"source":["import pandas as pd\n","\n","results_df = pd.DataFrame({\n","    \"Split\": [\"Train\", \"Validation\", \"Test\"],\n","    \"R2\": [\n","        eval_res[\"train_pred\"][\"r2\"],\n","        eval_res[\"val_pred\"][\"r2\"],\n","        eval_res[\"test_pred\"][\"r2\"],\n","    ],\n","    \"RMSE\": [\n","        eval_res[\"train_pred\"][\"rmse\"],\n","        eval_res[\"val_pred\"][\"rmse\"],\n","        eval_res[\"test_pred\"][\"rmse\"],\n","    ]\n","})\n","\n","results_df.to_csv(f\"data/{ticker}/Tp{tp}_eval_metrics.csv\", index=False)\n","print(f\"‚úÖ Saved R¬≤ and RMSE metrics to data/{ticker}/Tp{tp}_eval_metrics.csv\")\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
